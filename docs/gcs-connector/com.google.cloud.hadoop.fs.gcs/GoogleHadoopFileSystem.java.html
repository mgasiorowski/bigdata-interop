<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>GoogleHadoopFileSystem.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">coverage</a> &gt; <a href="../index.html" class="el_bundle">gcs-connector</a> &gt; <a href="index.source.html" class="el_package">com.google.cloud.hadoop.fs.gcs</a> &gt; <span class="el_source">GoogleHadoopFileSystem.java</span></div><h1>GoogleHadoopFileSystem.java</h1><pre class="source lang-java linenums">/*
 * Copyright 2013 Google Inc. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.cloud.hadoop.fs.gcs;

import static com.google.common.base.Preconditions.checkArgument;

import com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem;
import com.google.cloud.hadoop.gcsio.StorageResourceId;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.flogger.GoogleLogger;
import java.io.IOException;
import java.net.URI;
import org.apache.hadoop.fs.Path;

/**
 * GoogleHadoopFileSystem is a version of GoogleHadoopFileSystemBase which is rooted in a single
 * bucket at initialization time; in this case, Hadoop paths no longer correspond directly to
 * general GCS paths, and all Hadoop operations going through this FileSystem will never touch any
 * GCS bucket other than the bucket on which this FileSystem is rooted.
 *
 * &lt;p&gt;This implementation sacrifices a small amount of cross-bucket interoperability in favor of
 * more straightforward FileSystem semantics and compatibility with existing Hadoop applications. In
 * particular, it is not subject to bucket-naming constraints, and files are allowed to be placed in
 * root.
 */
public class GoogleHadoopFileSystem extends GoogleHadoopFileSystemBase {

<span class="fc" id="L42">  private static final GoogleLogger logger = GoogleLogger.forEnclosingClass();</span>

  // The bucket the file system is rooted in used for default values of:
  // -- working directory
  // -- user home directories (only for Hadoop purposes).
  private String rootBucket;

  /**
   * Constructs an instance of GoogleHadoopFileSystem; the internal
   * GoogleCloudStorageFileSystem will be set up with config settings when initialize() is called.
   */
  public GoogleHadoopFileSystem() {
<span class="fc" id="L54">    super();</span>
<span class="fc" id="L55">  }</span>

  /**
   * Constructs an instance of GoogleHadoopFileSystem using the provided
   * GoogleCloudStorageFileSystem; initialize() will not re-initialize it.
   */
  @VisibleForTesting
  GoogleHadoopFileSystem(GoogleCloudStorageFileSystem gcsfs) {
<span class="fc" id="L63">    super(gcsfs);</span>
<span class="fc" id="L64">  }</span>

  /** Sets and validates the root bucket. */
  @Override
  @VisibleForTesting
  protected void configureBuckets(GoogleCloudStorageFileSystem gcsFs) throws IOException {
<span class="fc" id="L70">    rootBucket = initUri.getAuthority();</span>
<span class="fc bfc" id="L71" title="All 2 branches covered.">    checkArgument(rootBucket != null, &quot;No bucket specified in GCS URI: %s&quot;, initUri);</span>
    // Validate root bucket name
<span class="fc" id="L73">    pathCodec.getPath(rootBucket, /* objectName= */ null, /* allowEmptyObjectName= */ true);</span>
<span class="fc" id="L74">    logger.atFine().log(</span>
        &quot;GHFS.configureBuckets: GoogleHadoopFileSystem root in bucket: %s&quot;, rootBucket);
<span class="fc" id="L76">  }</span>

  @Override
  protected void checkPath(Path path) {
    // Validate scheme
<span class="fc" id="L81">    super.checkPath(path);</span>
<span class="fc" id="L82">    URI uri = path.toUri();</span>
<span class="fc" id="L83">    String bucket = uri.getAuthority();</span>
    // Bucketless URIs will be qualified later
<span class="fc bfc" id="L85" title="All 4 branches covered.">    if (bucket == null || bucket.equals(rootBucket)) {</span>
<span class="fc" id="L86">      return;</span>
    }
<span class="fc" id="L88">    throw new IllegalArgumentException(</span>
<span class="fc" id="L89">        String.format(</span>
            &quot;Wrong bucket: %s, in path: %s, expected bucket: %s&quot;, bucket, path, rootBucket));
  }

  /**
   * Get the name of the bucket in which file system is rooted.
   */
  @VisibleForTesting
  String getRootBucketName() {
<span class="fc" id="L98">    return rootBucket;</span>
  }

  /**
   * Override to allow a homedir subpath which sits directly on our FileSystem root.
   */
  @Override
  protected String getHomeDirectorySubpath() {
<span class="fc" id="L106">    return &quot;user/&quot; + System.getProperty(&quot;user.name&quot;);</span>
  }

  /**
   * Validates GCS Path belongs to this file system. The bucket must
   * match the root bucket provided at initialization time.
   */
  @Override
  public Path getHadoopPath(URI gcsPath) {
<span class="fc" id="L115">    logger.atFine().log(&quot;GHFS.getHadoopPath: %s&quot;, gcsPath);</span>

    // Handle root. Delegate to getGcsPath on &quot;gs:/&quot; to resolve the appropriate gs://&lt;bucket&gt; URI.
<span class="fc bfc" id="L118" title="All 2 branches covered.">    if (gcsPath.equals(getGcsPath(getFileSystemRoot()))) {</span>
<span class="fc" id="L119">      return getFileSystemRoot();</span>
    }

<span class="fc" id="L122">    StorageResourceId resourceId = pathCodec.validatePathAndGetId(gcsPath, true);</span>

    // Unlike the global-rooted GHFS, gs:// has no meaning in the bucket-rooted world.
<span class="pc bpc" id="L125" title="1 of 2 branches missed.">    checkArgument(!resourceId.isRoot(), &quot;Missing authority in gcsPath '%s'&quot;, gcsPath);</span>
<span class="fc" id="L126">    checkArgument(</span>
<span class="fc" id="L127">        resourceId.getBucketName().equals(rootBucket),</span>
        &quot;Authority of URI '%s' doesn't match root bucket '%s'&quot;,
<span class="fc" id="L129">        resourceId.getBucketName(), rootBucket);</span>

<span class="fc" id="L131">    Path hadoopPath = new Path(getScheme() + &quot;://&quot; + rootBucket + '/' + resourceId.getObjectName());</span>
<span class="fc" id="L132">    logger.atFine().log(&quot;GHFS.getHadoopPath: %s -&gt; %s&quot;, gcsPath, hadoopPath);</span>
<span class="fc" id="L133">    return hadoopPath;</span>
  }

  /**
   * Translates a &quot;gs:/&quot; style hadoopPath (or relative path which is not fully-qualified) into
   * the appropriate GCS path which is compatible with the underlying GcsFs or gsutil.
   */
  @Override
  public URI getGcsPath(Path hadoopPath) {
<span class="fc" id="L142">    logger.atFine().log(&quot;GHFS.getGcsPath: %s&quot;, hadoopPath);</span>

    // Convert to fully qualified absolute path; the Path object will callback to get our current
    // workingDirectory as part of fully resolving the path.
<span class="fc" id="L146">    Path resolvedPath = makeQualified(hadoopPath);</span>

<span class="fc" id="L148">    String objectName = resolvedPath.toUri().getPath();</span>
<span class="pc bpc" id="L149" title="2 of 4 branches missed.">    if (objectName != null &amp;&amp; resolvedPath.isAbsolute()) {</span>
      // Strip off leading '/' because GoogleCloudStorageFileSystem.getPath appends it explicitly
      // between bucket and objectName.
<span class="fc" id="L152">      objectName = objectName.substring(1);</span>
    }

    // Construct GCS path uri.
<span class="fc" id="L156">    URI gcsPath = pathCodec.getPath(rootBucket, objectName, true);</span>
<span class="fc" id="L157">    logger.atFine().log(&quot;GHFS.getGcsPath: %s -&gt; %s&quot;, hadoopPath, gcsPath);</span>
<span class="fc" id="L158">    return gcsPath;</span>
  }

  /**
   * As the global-rooted FileSystem, our hadoop-path &quot;scheme&quot; is exactly equal to the general
   * GCS scheme.
   */
  @Override
  public String getScheme() {
<span class="fc" id="L167">    return GoogleCloudStorageFileSystem.SCHEME;</span>
  }

  @Override
  public Path getFileSystemRoot() {
<span class="fc" id="L172">    return new Path(getScheme() + &quot;://&quot; + rootBucket + '/');</span>
  }

  /**
   * Gets the default value of working directory.
   */
  @Override
  public Path getDefaultWorkingDirectory() {
<span class="nc" id="L180">    return getFileSystemRoot();</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.4.201905082037</span></div></body></html>